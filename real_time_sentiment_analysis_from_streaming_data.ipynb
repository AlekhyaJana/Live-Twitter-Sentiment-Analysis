{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify live stream data for a given amount of time\n",
    "#neg,pos,neu\n",
    "#store the tweets\n",
    "my_tweets=list()\n",
    "result=[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "pickle_in=open(\"word_bucket.pickle\",\"rb\")\n",
    "word_bucket=pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "model_pred=tf.keras.models.load_model(\"DL_sentimentanalysis_new.h5\")\n",
    "output=[\"Negative\",\"Positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing_set(data,vocab):\n",
    "    dataset=[]\n",
    "    for desc in data:\n",
    "        vector=[]\n",
    "        for d in vocab:\n",
    "            if d in desc:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        dataset.append(vector)\n",
    "    return(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords=set(stopwords.words('english'))\n",
    "def remove_punctuation(string):\n",
    "    punctuations='''!()[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    for chars in string:\n",
    "        if chars in punctuations:\n",
    "            string=string.replace(chars,\"\")\n",
    "    return(string)\n",
    "\n",
    "def remove_alpha_numeric(string):\n",
    "    line=list()\n",
    "    for words in string.split(\" \"):\n",
    "        if words.isalpha() == True and words not in stopwords:\n",
    "            line.append(words)\n",
    "    return(line)\n",
    "\n",
    "def lower_case(string):\n",
    "    S_list=list()\n",
    "    for s in string:\n",
    "        S_list.append(s.lower())     \n",
    "    return(S_list)\n",
    "\n",
    "def cleaning_text(data):\n",
    "    cleaned_data=list()\n",
    "    for d in data:\n",
    "        d1=remove_punctuation(d)\n",
    "        d1=remove_alpha_numeric(d1)\n",
    "        d1=lower_case(d1)\n",
    "        cleaned_data.append(d1)\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string=input(\"Type the hastags whose sentiment want to be analysed\")\n",
    "max_count=int(input(\"How many english tweets do you wanna consider ?\"))\n",
    "hashtag_list=[string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "#define a processing class\n",
    "class Authenticator():\n",
    "    def authenticate(self,access_info):\n",
    "        \n",
    "        #create instance of OAuthHandler class\n",
    "        auth = tweepy.OAuthHandler(access_info[0],access_info[1])\n",
    "        \n",
    "        #use the instancce for setting the access tokens\n",
    "        auth.set_access_token(access_info[2],access_info[3])\n",
    "        \n",
    "        return auth\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processor():\n",
    "    def __init__(self):\n",
    "        #create an object of Authenticator class\n",
    "        self.twitter_authenticate=Authenticator()\n",
    "        \n",
    "    def get_info(self,access_info,hashtag_list):\n",
    "        \n",
    "        #authenticate credentials\n",
    "        auth=self.twitter_authenticate.authenticate(access_info)\n",
    "        \n",
    "        #create an instance of API class\n",
    "        api = tweepy.API(auth)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #create an instance of MyStreamListener class\n",
    "        myStreamListener = MyStreamListener()\n",
    "        \n",
    "        #create an objects of Stream class, which takes two parameters\n",
    "        myStream = tweepy.Stream(auth = api.auth, listener=myStreamListener)\n",
    "        \n",
    "        #filter data based on hashtags\n",
    "        myStream.filter(track=hashtag_list)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#override tweepy.StreamListener class, to handle the tweets.\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    def __init__(self,):\n",
    "        pass\n",
    "        \n",
    "    def on_data(self, tweets):\n",
    "        tweets_dic = json.loads(tweets)\n",
    "        #print(\"NEXT TWEET------>\")\n",
    "        stringg=tweets_dic[\"text\"]\n",
    "        if len(my_tweets)<max_count:\n",
    "            string=cleaning_text([stringg])\n",
    "            test=preparing_set(string,word_bucket)\n",
    "            test=np.array(test).reshape(-1,len(word_bucket))\n",
    "            if np.count_nonzero(test)!=0:\n",
    "                my_tweets.append(stringg)\n",
    "                prediction=model_pred.predict_proba(test)\n",
    "                if prediction[0][np.argmax(prediction)]>=0.60:\n",
    "                    result[np.argmax(prediction)]=result[np.argmax(prediction)]+1\n",
    "                else:\n",
    "                    result[2]=result[2]+1\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    def on_error(self, status):\n",
    "        #for the continuous consumption of data\n",
    "        if status==420:\n",
    "            return False\n",
    "        print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key=<paste ur credentials>\n",
    "consumer_secret=<paste ur credentials>\n",
    "access_token=<paste ur credentials>\n",
    "access_token_secret=<paste ur credentials>\n",
    "access_info=list()\n",
    "access_info.append(consumer_key)\n",
    "access_info.append(consumer_secret)\n",
    "access_info.append(access_token)\n",
    "access_info.append(access_token_secret)\n",
    "processor_obj=Processor()\n",
    "processor_obj.get_info(access_info,hashtag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create graph\n",
    "import matplotlib.pyplot as plt\n",
    "plt.pie(result,autopct='%1.0f%%',)\n",
    "plt.legend([\"neagtive\",\"positive\",\"neutral\"],loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you wanna view the tweets\n",
    "#for twts in my_tweets:\n",
    "    #print(twts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
