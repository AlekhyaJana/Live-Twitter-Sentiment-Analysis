{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def extract_file(filename):\n",
    "    \n",
    "    # initializing rows list \n",
    "    rows = [] \n",
    "\n",
    "    # reading csv file \n",
    "    with open(filename, 'r') as csvfile: \n",
    "\n",
    "        # creating a csv reader object \n",
    "        csvreader = csv.reader(csvfile) \n",
    "\n",
    "\n",
    "        # extracting each data row one by one \n",
    "        for row in csvreader: \n",
    "            rows.append(row) \n",
    "\n",
    "        # get total number of rows \n",
    "        print(\"Total no. of rows: %d\"%(csvreader.line_num)) \n",
    "        \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cleaning1(Data):\n",
    "    New_data=list()\n",
    "    for d in Data:\n",
    "        New_data.append(d[1:])\n",
    "    return New_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords=set(stopwords.words('english'))\n",
    "def remove_punctuation(string):\n",
    "    punctuations='''!()[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    for chars in string:\n",
    "        if chars in punctuations:\n",
    "            string=string.replace(chars,\"\")\n",
    "    return(string)\n",
    "\n",
    "def remove_alpha_numeric(string):\n",
    "    line=list()\n",
    "    for words in string.split(\" \"):\n",
    "        if words.isalpha() == True and words not in stopwords:\n",
    "            line.append(words)\n",
    "    return(line)\n",
    "\n",
    "def lower_case(string):\n",
    "    S_list=list()\n",
    "    for s in string:\n",
    "        S_list.append(s.lower())     \n",
    "    return(S_list)\n",
    "\n",
    "def cleaning_text(data):\n",
    "    cleaned_data=list()\n",
    "    for d in data:\n",
    "        d1=remove_punctuation(d[1])\n",
    "        d1=remove_alpha_numeric(d1)\n",
    "        d1=lower_case(d1)\n",
    "        cleaned_data.append([int(d[0]),d1])\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocubulary(data,threshold):\n",
    "    Bag_of_words=list()\n",
    "    for d in data.keys():\n",
    "        if d not in Bag_of_words:\n",
    "            if float(data[d])<=0.6 and float(data[d])>=0.3:\n",
    "                Bag_of_words.append(d)\n",
    "    return(Bag_of_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"train.csv\\\\train.csv\"\n",
    "Data=extract_file(filename)\n",
    "print(Data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqd_data=Cleaning1(Data[1:])\n",
    "print(reqd_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data=cleaning_text(reqd_data)\n",
    "print(cleaned_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set=list()\n",
    "for lab,dat in cleaned_data:\n",
    "    word_set.append(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import math  \n",
    "def document_freq(words,document_list):\n",
    "    freq=0\n",
    "    for documents in document_list: \n",
    "        if words in documents:\n",
    "            freq+=1\n",
    "    return freq\n",
    "\n",
    "def inverse_document_freq(document_list):\n",
    "    corpus=dict()\n",
    "    for documents in tqdm(document_list): \n",
    "        for words in documents:\n",
    "            if words not in corpus.keys():\n",
    "                DF=document_freq(words,document_list)\n",
    "                IDF=math.log10(len(document_list))-math.log10(DF)\n",
    "                corpus[words]=IDF\n",
    "    return(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logs=inverse_document_freq(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_bucket=list()\n",
    "for keys in logs:\n",
    "    if logs[keys]>=1.5 and logs[keys]<=3 and len(keys)>3 and keys not in stopwords:\n",
    "        word_bucket.append(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset\n",
    "def preparing_set(data,vocab):\n",
    "    dataset=[]\n",
    "    for lab,desc in data:\n",
    "        vector=[]\n",
    "        for d in vocab:\n",
    "            if d in desc:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        dataset.append([vector,lab])\n",
    "    return(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset=preparing_set(cleaned_data,word_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "random.shuffle(Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "Y=[]\n",
    "for data,lab in Dataset:\n",
    "    X.append(data)\n",
    "    Y.append(lab)\n",
    "X=np.array(X).reshape(-1,len(word_bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "#if you wanna use SVM\n",
    "svm_model_linear = SVC(kernel = 'linear', C = 1).fit(X_train, Y_train) \n",
    "svm_predictions = svm_model_linear.predict(X_test) \n",
    "accuracy = svm_model_linear.score(X_test, Y_test) \n",
    "print(accuracy)\n",
    "cm = confusion_matrix(Y_test, svm_predictions) \n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.show()\n",
    "import pickle\n",
    "modelname='sentimentanalysis_svm'\n",
    "with open (modelname,'wb') as file:\n",
    "    pickle.dump(svm_model_linear,file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or use dense neural networ\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import numpy as np\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(32, activation='tanh', input_shape=(len(word_bucket),)))\n",
    "\n",
    "model.add(Dense(256,activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(128,activation='tanh'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(64,activation='tanh'))\n",
    "\n",
    "model.add(Dense(2,activation='sigmoid'))\n",
    "adam=Adam(lr=0.00005)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer=adam,metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "model.fit(X_train,Y_train,epochs=25,validation_data=(X_test,Y_test),batch_size=5,callbacks=None)\n",
    "\n",
    "model.save(\"DL_sentimentanalysis_new.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out=open(\"word_bucket.pickle\",\"wb\")\n",
    "pickle.dump(word_bucket,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
